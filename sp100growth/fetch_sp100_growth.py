#!/usr/bin/env python3
"""
SP100 Growth Extractor - SEC EDGAR data extractor for annual & quarterly financials.

Pulls annual (10-K) and quarterly (10-Q) revenue and EPS data from SEC EDGAR,
with multi-source fallback for complete data coverage. Calculates YoY growth, TTM, and CAGR.

This collector fetches fundamental growth metrics for S&P 100 companies:
- Annual revenue and EPS (from 10-K filings)
- Quarterly revenue and EPS (from 10-Q filings)
- Year-over-Year growth rates
- Trailing Twelve Months (TTM) metrics
- 3-year and 5-year CAGR

Data Sources (in priority order):
- SEC EDGAR: Primary source for all annual and quarterly data
- yfinance (Yahoo Finance): First fallback for annual data
- Alpha Vantage API: Second fallback for annual data
- Finnhub API: Final fallback for annual data, also used for quarterly fallback

The fallback hierarchy ensures maximum data coverage, especially for:
- Financial sector companies (banks use different revenue concepts)
- Companies with non-standard SEC filings
- Recent quarters before SEC filings are available

Usage:
    python fetch_sp100_growth.py                    # Use S&P 100 universe
    python fetch_sp100_growth.py --output ./output  # Custom output directory
"""
from __future__ import annotations

import argparse
import json
import logging
import os
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict, field
from datetime import datetime

import requests
import yaml
import pandas as pd

# Use the secedgar library
from secedgar.cik_lookup import get_cik_map
from secedgar.core.rest import get_company_facts

try:
    from secedgar.exceptions import EDGARQueryError
except ImportError:
    class EDGARQueryError(Exception):
        pass

# Add parent directory to path for shared imports
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Import S&P 100 universe from shared module
from shared.sp100_universe import fetch_sp100_tickers


# ============================================================================
# Data Classes
# ============================================================================

@dataclass
class ValidationResult:
    """Result of cross-validating a value across multiple fallback sources."""
    value: Optional[float] = None
    status: str = "none"  # "validated", "averaged", "discrepancy", "single_source", "none"
    sources_compared: List[str] = field(default_factory=list)
    source_values: Dict[str, float] = field(default_factory=dict)
    discrepancy_pct: Optional[float] = None  # Max percentage difference between sources


@dataclass
class AnnualRecord:
    """Single year of financial data."""
    fiscal_year_end: str
    revenue: Optional[float] = None
    eps_diluted: Optional[float] = None
    revenue_concept: Optional[str] = None
    eps_concept: Optional[str] = None
    # Validation fields for fallback data
    revenue_validation: Optional[str] = None  # "validated", "averaged", "discrepancy", "single_source", None
    eps_validation: Optional[str] = None
    revenue_sources: Optional[List[str]] = None  # Sources compared for revenue
    eps_sources: Optional[List[str]] = None  # Sources compared for EPS
    revenue_discrepancy_pct: Optional[float] = None  # Max % diff between sources
    eps_discrepancy_pct: Optional[float] = None


@dataclass
class QuarterlyRecord:
    """Single quarter of financial data."""
    fiscal_quarter_end: str
    revenue: Optional[float] = None
    eps_diluted: Optional[float] = None
    source: str = "sec"  # "sec" or "finnhub"


@dataclass
class TTMMetrics:
    """Trailing Twelve Months metrics calculated from quarterly data."""
    revenue: Optional[float] = None
    eps_diluted: Optional[float] = None
    revenue_yoy: Optional[float] = None  # TTM vs TTM from 4 quarters ago
    eps_yoy: Optional[float] = None
    as_of_quarter: Optional[str] = None  # End date of most recent quarter
    source: str = "sec"  # "sec", "finnhub", or "annual_fallback"


@dataclass
class GrowthMetrics:
    """Year-over-year growth calculations."""
    revenue_yoy: Dict[str, Optional[float]]  # {"2024": 0.05, "2023": 0.08}
    eps_yoy: Dict[str, Optional[float]]
    ttm: Optional[TTMMetrics] = None
    revenue_cagr_3yr: Optional[float] = None
    eps_cagr_3yr: Optional[float] = None
    revenue_cagr_5yr: Optional[float] = None
    eps_cagr_5yr: Optional[float] = None


@dataclass
class CompanyData:
    """Complete extracted data for one company."""
    ticker: str
    cik: str
    company_name: Optional[str]
    extracted_at: str
    annual_data: List[AnnualRecord]
    quarterly_data: List[QuarterlyRecord]
    growth: GrowthMetrics
    errors: List[str]


# ============================================================================
# Configuration
# ============================================================================

@dataclass
class Config:
    user_agent: str
    years_to_fetch: int
    quarters_to_fetch: int
    concepts: Dict[str, List[str]]
    output_dir: Path
    output_filename: str
    indent: int
    finnhub_enabled: bool
    finnhub_api_key: str
    yfinance_enabled: bool
    alphavantage_enabled: bool
    alphavantage_api_key: str
    fmp_enabled: bool  # Financial Modeling Prep (tiebreaker for discrepancies)
    fmp_api_key: str

    @staticmethod
    def from_yaml(path: str) -> "Config":
        with open(path, "r") as f:
            raw = yaml.safe_load(f)
        
        sec = raw.get("sec", {})
        output = raw.get("output", {})
        finnhub = raw.get("finnhub", {})
        yfinance = raw.get("yfinance", {})
        alphavantage = raw.get("alphavantage", {})
        fmp = raw.get("fmp", {})
        
        # Read API keys from environment variables (standard pattern)
        finnhub_api_key = os.environ.get("FINNHUB_API_KEY", "")
        alphavantage_api_key = os.environ.get("ALPHA_VANTAGE_API_KEY", "")
        fmp_api_key = os.environ.get("FMP_API_KEY", "")
        
        return Config(
            user_agent=sec.get("user_agent", ""),
            years_to_fetch=raw.get("years_to_fetch", 6),
            quarters_to_fetch=raw.get("quarters_to_fetch", 8),
            concepts=raw.get("concepts", {}),
            output_dir=Path(output.get("directory", "./output")),
            output_filename=output.get("filename", "sp100growth.json"),
            indent=output.get("indent", 2),
            finnhub_enabled=finnhub.get("enabled", False),
            finnhub_api_key=finnhub_api_key,
            yfinance_enabled=yfinance.get("enabled", True),
            alphavantage_enabled=alphavantage.get("enabled", False),
            alphavantage_api_key=alphavantage_api_key,
            fmp_enabled=fmp.get("enabled", False),  # Disabled by default - only enable when needed
            fmp_api_key=fmp_api_key,
        )


# ============================================================================
# SEC Data Fetching
# ============================================================================

def load_ticker_to_cik(user_agent: str) -> Dict[str, str]:
    """Load ticker -> CIK mapping from SEC."""
    try:
        m = get_cik_map(user_agent=user_agent)["ticker"]
        return {t.upper(): str(cik).zfill(10) for t, cik in m.items() if t and cik}
    except Exception as e:
        print(f"[warn] Failed to load CIK map: {e}")
        return {}


def fetch_company_facts(ticker: str, user_agent: str) -> Optional[dict]:
    """Fetch company facts JSON from SEC EDGAR."""
    try:
        facts_map = get_company_facts(lookups=[ticker], user_agent=user_agent)
        return facts_map.get(ticker) or facts_map.get(ticker.upper()) or next(iter(facts_map.values()), None)
    except EDGARQueryError as e:
        print(f"[warn] SEC query error for {ticker}: {e}")
        return None
    except Exception as e:
        print(f"[warn] Failed to fetch {ticker}: {e}")
        return None


# ============================================================================
# Finnhub Fallback
# ============================================================================

def finnhub_quarterly_financials(symbol: str, api_key: str, timeout: int = 15) -> pd.DataFrame:
    """Fetch quarterly revenue and EPS from Finnhub."""
    if not api_key:
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted"])
    
    base = "https://finnhub.io/api/v1"
    rows = {}
    
    # Income statement for revenue and EPS
    try:
        url = f"{base}/stock/financials?symbol={symbol}&statement=ic&freq=quarterly&token={api_key}"
        r = requests.get(url, timeout=timeout)
        if r.status_code == 200:
            js = r.json() or {}
            for item in (js.get("data") or []):
                end = item.get("period") or item.get("endDate") or item.get("end")
                if not end:
                    continue
                rec = rows.setdefault(end, {"end": end})
                
                # Revenue
                rev = item.get("revenue") or item.get("totalRevenue") or item.get("Revenue")
                if rev is not None:
                    try:
                        rec["revenue"] = float(rev)
                    except:
                        pass
                
                # EPS Diluted
                eps = item.get("epsdiluted") or item.get("epsDiluted") or item.get("EPSDiluted")
                if eps is not None:
                    try:
                        rec["eps_diluted"] = float(eps)
                    except:
                        pass
    except Exception:
        pass
    
    # Also try earnings calendar for EPS if not found
    if not any("eps_diluted" in r for r in rows.values()):
        try:
            url = f"{base}/calendar/earnings?symbol={symbol}&from=2020-01-01&to=2100-01-01&token={api_key}"
            r = requests.get(url, timeout=timeout)
            if r.status_code == 200:
                js = r.json() or {}
                for it in (js.get("earningsCalendar") or []):
                    end = it.get("date") or it.get("period")
                    eps = it.get("epsActual") or it.get("reportedEPS")
                    if end and eps is not None:
                        rec = rows.setdefault(end, {"end": end})
                        try:
                            rec["eps_diluted"] = float(eps)
                        except:
                            pass
        except Exception:
            pass
    
    if not rows:
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted"])
    
    df = pd.DataFrame(list(rows.values()))
    df = df.sort_values("end", ascending=False).drop_duplicates("end", keep="first")
    return df


# ============================================================================
# Multi-Source Validation (Consensus-Based)
# ============================================================================

def _values_match(v1: float, v2: float, tolerance_pct: float) -> bool:
    """Check if two values are within tolerance percentage of each other."""
    if v1 == 0 and v2 == 0:
        return True
    if v1 == 0 or v2 == 0:
        return False
    max_val = max(abs(v1), abs(v2))
    diff_pct = abs(v1 - v2) / max_val * 100
    return diff_pct <= tolerance_pct


def validate_across_sources(
    source_values: Dict[str, Optional[float]],
    tolerance_pct: float = 5.0
) -> ValidationResult:
    """
    Cross-validate a value using consensus-based voting.
    
    Logic:
    - If 2+ sources agree (within tolerance): Use the consensus value, status = "validated"
    - If all sources differ: Average all values, status = "discrepancy"
    - If only 1 source: Use that value, status = "single_source"
    
    Args:
        source_values: Dict mapping source name to value (e.g., {"yfinance": 10.2, "alphavantage": 10.3, "fmp": 10.2})
        tolerance_pct: Maximum percentage difference to consider values as "matching" (default 5%)
    
    Returns:
        ValidationResult with consensus value and validation status
    """
    # Filter out None values
    valid_sources = {k: v for k, v in source_values.items() if v is not None}
    
    if not valid_sources:
        return ValidationResult(status="none")
    
    if len(valid_sources) == 1:
        source_name, value = list(valid_sources.items())[0]
        return ValidationResult(
            value=value,
            status="single_source",
            sources_compared=[source_name],
            source_values=valid_sources
        )
    
    # Calculate overall discrepancy for reporting
    values = list(valid_sources.values())
    max_val = max(values)
    min_val = min(values)
    discrepancy_pct = ((max_val - min_val) / max_val) * 100 if max_val > 0 else 0
    
    # With only 2 sources, check if they match
    if len(valid_sources) == 2:
        source_names = list(valid_sources.keys())
        v1, v2 = values[0], values[1]
        if _values_match(v1, v2, tolerance_pct):
            return ValidationResult(
                value=v1,  # Use first source
                status="validated",
                sources_compared=source_names,
                source_values=valid_sources,
                discrepancy_pct=round(discrepancy_pct, 2)
            )
        else:
            # 2 sources disagree - average and flag
            avg_value = sum(values) / len(values)
            return ValidationResult(
                value=avg_value,
                status="discrepancy",
                sources_compared=source_names,
                source_values=valid_sources,
                discrepancy_pct=round(discrepancy_pct, 2)
            )
    
    # With 3+ sources, find consensus (2+ sources that agree)
    source_names = list(valid_sources.keys())
    
    # Check all pairs to find consensus groups
    for i, (name1, val1) in enumerate(valid_sources.items()):
        matching_sources = [name1]
        matching_values = [val1]
        
        for j, (name2, val2) in enumerate(valid_sources.items()):
            if i != j and _values_match(val1, val2, tolerance_pct):
                matching_sources.append(name2)
                matching_values.append(val2)
        
        # If 2+ sources agree, we have consensus
        if len(matching_sources) >= 2:
            # Use the average of matching values for precision
            consensus_value = sum(matching_values) / len(matching_values)
            return ValidationResult(
                value=consensus_value,
                status="validated",
                sources_compared=source_names,
                source_values=valid_sources,
                discrepancy_pct=round(discrepancy_pct, 2)
            )
    
    # No consensus found - all sources differ, average and flag as discrepancy
    avg_value = sum(values) / len(values)
    return ValidationResult(
        value=avg_value,
        status="discrepancy",
        sources_compared=source_names,
        source_values=valid_sources,
        discrepancy_pct=round(discrepancy_pct, 2)
    )


def gather_all_fallback_data(
    ticker: str, 
    config,
    years_to_fetch: int = 6
) -> Dict[str, pd.DataFrame]:
    """
    Fetch data from fallback sources in priority order.
    
    Priority order:
    1. yfinance: Free, no API key required (primary fallback)
    2. Alpha Vantage: Requires ALPHA_VANTAGE_API_KEY (secondary fallback)
    
    FMP is disabled by default due to API limits (250 calls/day).
    Finnhub is reserved for quarterly fallback only.
    
    Returns dict mapping source name to DataFrame with columns: end, revenue, eps_diluted
    """
    all_sources = {}
    
    # yfinance (primary fallback - free, no API key needed)
    if config.yfinance_enabled:
        yf_df = yfinance_annual_financials(ticker, years_to_fetch)
        if not yf_df.empty:
            all_sources["yfinance"] = yf_df
    
    # Alpha Vantage (secondary fallback)
    if config.alphavantage_enabled and config.alphavantage_api_key:
        av_df = alphavantage_annual_financials(ticker, config.alphavantage_api_key, years_to_fetch)
        if not av_df.empty:
            all_sources["alphavantage"] = av_df
    
    # FMP is optional - disabled by default due to 250 calls/day limit
    # Only enable if you have a premium FMP account
    if config.fmp_enabled and config.fmp_api_key:
        fmp_df = fmp_annual_financials(ticker, config.fmp_api_key, years_to_fetch)
        if not fmp_df.empty:
            all_sources["fmp"] = fmp_df
    
    return all_sources


def get_fallback_value(
    year_date: str,
    all_sources: Dict[str, pd.DataFrame],
    metric: str,  # "revenue" or "eps_diluted"
) -> ValidationResult:
    """
    Get a value from fallback sources using priority order.
    
    Priority: yfinance > alphavantage > fmp
    
    If only one source has data, use it (status = "single_source").
    If multiple sources have data and agree (within 5%), use yfinance (status = "validated").
    If multiple sources disagree (>5% diff), use yfinance but flag as "discrepancy".
    
    Args:
        year_date: Fiscal year end date (e.g., "2024-12-31")
        all_sources: Dict from gather_all_fallback_data()
        metric: "revenue" or "eps_diluted"
    
    Returns:
        ValidationResult with value and status
    """
    year_prefix = year_date[:4]  # Extract year for fuzzy matching
    source_values = {}
    
    # Collect values from all sources
    for source_name, df in all_sources.items():
        # Try exact match first
        match = df[df["end"] == year_date]
        if match.empty:
            # Try year match (handles different fiscal year endings)
            match = df[df["end"].str.startswith(year_prefix)]
        
        if not match.empty:
            val = match.iloc[0].get(metric)
            if pd.notna(val):
                source_values[source_name] = float(val)
    
    if not source_values:
        return ValidationResult(status="none")
    
    # Priority order for selecting the value
    priority_order = ["yfinance", "alphavantage", "fmp"]
    selected_source = None
    selected_value = None
    
    for source in priority_order:
        if source in source_values:
            selected_source = source
            selected_value = source_values[source]
            break
    
    if selected_value is None:
        return ValidationResult(status="none")
    
    # Determine validation status
    source_names = list(source_values.keys())
    
    if len(source_values) == 1:
        # Only one source has data
        return ValidationResult(
            value=selected_value,
            status="single_source",
            sources_compared=source_names,
            source_values=source_values
        )
    
    # Multiple sources - check if they agree
    values = list(source_values.values())
    max_val = max(values)
    min_val = min(values)
    discrepancy_pct = ((max_val - min_val) / max_val) * 100 if max_val > 0 else 0
    
    if discrepancy_pct <= 5.0:
        # Sources agree - use primary source (yfinance)
        return ValidationResult(
            value=selected_value,
            status="validated",
            sources_compared=source_names,
            source_values=source_values,
            discrepancy_pct=round(discrepancy_pct, 2)
        )
    else:
        # Sources disagree - use primary source but flag as discrepancy
        return ValidationResult(
            value=selected_value,
            status="discrepancy",
            sources_compared=source_names,
            source_values=source_values,
            discrepancy_pct=round(discrepancy_pct, 2)
        )


def cross_validate_fallback_year(
    year_date: str,
    all_sources: Dict[str, pd.DataFrame],
    metric: str,  # "revenue" or "eps_diluted"
    tolerance_pct: float = 5.0
) -> ValidationResult:
    """
    Get a value from fallback sources with status tracking.
    
    This is a wrapper around get_fallback_value() for backward compatibility.
    Uses yfinance as primary, Alpha Vantage as secondary.
    """
    return get_fallback_value(year_date, all_sources, metric)


# ============================================================================
# yfinance Fallback (Annual)
# ============================================================================

def yfinance_annual_financials(symbol: str, years_to_fetch: int = 6) -> pd.DataFrame:
    """
    Fetch annual revenue and EPS from Yahoo Finance.
    Returns DataFrame with columns: end, revenue, eps_diluted, source
    """
    try:
        import yfinance as yf
    except ImportError:
        print(f"[warn] yfinance not installed, skipping yfinance fallback")
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
    
    try:
        ticker = yf.Ticker(symbol)
        income_stmt = ticker.income_stmt
        
        if income_stmt is None or income_stmt.empty:
            return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
        
        rows = []
        for col in income_stmt.columns[:years_to_fetch]:
            # col is a Timestamp for the fiscal year end
            end_date = col.strftime("%Y-%m-%d")
            
            # Revenue - try multiple field names
            revenue = None
            for rev_field in ["Total Revenue", "Revenue", "Total Operating Revenue", "Gross Revenue"]:
                if rev_field in income_stmt.index:
                    val = income_stmt.loc[rev_field, col]
                    if pd.notna(val):
                        revenue = float(val)
                        break
            
            # EPS Diluted - try multiple field names
            eps = None
            for eps_field in ["Diluted EPS", "Basic EPS", "EPS"]:
                if eps_field in income_stmt.index:
                    val = income_stmt.loc[eps_field, col]
                    if pd.notna(val):
                        eps = float(val)
                        break
            
            if revenue is not None or eps is not None:
                rows.append({
                    "end": end_date,
                    "revenue": revenue,
                    "eps_diluted": eps,
                    "source": "yfinance"
                })
        
        if not rows:
            return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
        
        df = pd.DataFrame(rows)
        df = df.sort_values("end", ascending=False).drop_duplicates("end", keep="first")
        return df
        
    except Exception as e:
        print(f"[warn] yfinance error for {symbol}: {e}")
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])


# ============================================================================
# yfinance Fallback (Quarterly)
# ============================================================================

def yfinance_quarterly_financials(symbol: str, quarters_to_fetch: int = 8) -> pd.DataFrame:
    """
    Fetch quarterly revenue and EPS from Yahoo Finance.
    Returns DataFrame with columns: end, revenue, eps_diluted, source
    
    This is a key fallback for quarterly data when SEC EDGAR is missing values.
    yfinance provides quarterly data for most companies including:
    - Revenue (Total Revenue)
    - Diluted EPS
    
    Example companies that benefit: GOOGL, V (Visa), BRK-B
    """
    try:
        import yfinance as yf
    except ImportError:
        print(f"[warn] yfinance not installed, skipping yfinance quarterly fallback")
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
    
    try:
        ticker = yf.Ticker(symbol)
        q_income = ticker.quarterly_income_stmt
        
        if q_income is None or q_income.empty:
            return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
        
        rows = []
        for col in q_income.columns[:quarters_to_fetch]:
            # col is a Timestamp for the fiscal quarter end
            end_date = col.strftime("%Y-%m-%d")
            
            # Revenue - try multiple field names
            revenue = None
            for rev_field in ["Total Revenue", "Revenue", "Total Operating Revenue", "Gross Revenue"]:
                if rev_field in q_income.index:
                    val = q_income.loc[rev_field, col]
                    if pd.notna(val):
                        revenue = float(val)
                        break
            
            # EPS Diluted - try multiple field names
            eps = None
            for eps_field in ["Diluted EPS", "Basic EPS", "EPS"]:
                if eps_field in q_income.index:
                    val = q_income.loc[eps_field, col]
                    if pd.notna(val):
                        eps = float(val)
                        break
            
            if revenue is not None or eps is not None:
                rows.append({
                    "end": end_date,
                    "revenue": revenue,
                    "eps_diluted": eps,
                    "source": "yfinance"
                })
        
        if not rows:
            return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
        
        df = pd.DataFrame(rows)
        df = df.sort_values("end", ascending=False).drop_duplicates("end", keep="first")
        return df
        
    except Exception as e:
        print(f"[warn] yfinance quarterly error for {symbol}: {e}")
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])


# ============================================================================
# Alpha Vantage Fallback (Annual)
# ============================================================================

def alphavantage_annual_financials(symbol: str, api_key: str, years_to_fetch: int = 6, timeout: int = 15) -> pd.DataFrame:
    """
    Fetch annual revenue and EPS from Alpha Vantage.
    Returns DataFrame with columns: end, revenue, eps_diluted, source
    """
    if not api_key:
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
    
    base = "https://www.alphavantage.co/query"
    rows = {}
    
    # Income statement for revenue and EPS
    try:
        url = f"{base}?function=INCOME_STATEMENT&symbol={symbol}&apikey={api_key}"
        r = requests.get(url, timeout=timeout)
        if r.status_code == 200:
            js = r.json()
            for report in (js.get("annualReports") or [])[:years_to_fetch]:
                fiscal_end = report.get("fiscalDateEnding")
                if not fiscal_end:
                    continue
                
                rec = rows.setdefault(fiscal_end, {"end": fiscal_end, "source": "alphavantage"})
                
                # Revenue
                rev = report.get("totalRevenue")
                if rev and rev != "None":
                    try:
                        rec["revenue"] = float(rev)
                    except:
                        pass
                
                # EPS (Alpha Vantage earnings endpoint)
    except Exception as e:
        print(f"[warn] Alpha Vantage income statement error for {symbol}: {e}")
    
    # Also fetch earnings for EPS
    try:
        url = f"{base}?function=EARNINGS&symbol={symbol}&apikey={api_key}"
        r = requests.get(url, timeout=timeout)
        if r.status_code == 200:
            js = r.json()
            for report in (js.get("annualEarnings") or [])[:years_to_fetch]:
                fiscal_end = report.get("fiscalDateEnding")
                if not fiscal_end:
                    continue
                
                rec = rows.setdefault(fiscal_end, {"end": fiscal_end, "source": "alphavantage"})
                
                eps = report.get("reportedEPS")
                if eps and eps != "None":
                    try:
                        rec["eps_diluted"] = float(eps)
                    except:
                        pass
    except Exception as e:
        print(f"[warn] Alpha Vantage earnings error for {symbol}: {e}")
    
    if not rows:
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
    
    df = pd.DataFrame(list(rows.values()))
    df = df.sort_values("end", ascending=False).drop_duplicates("end", keep="first")
    
    # Filter out records where revenue is NULL - these are typically TTM/LTM values
    # from the EARNINGS endpoint that don't have corresponding annual revenue data.
    # This prevents phantom records like "2025-09-30" with EPS but no revenue.
    df = df[df["revenue"].notna()].copy()
    
    return df


# ============================================================================
# Finnhub Annual Fallback
# ============================================================================

def finnhub_annual_financials(symbol: str, api_key: str, years_to_fetch: int = 6, timeout: int = 15) -> pd.DataFrame:
    """
    Fetch annual revenue and EPS from Finnhub.
    Returns DataFrame with columns: end, revenue, eps_diluted, source
    """
    if not api_key:
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
    
    base = "https://finnhub.io/api/v1"
    rows = {}
    
    # Income statement for revenue (annual frequency)
    try:
        url = f"{base}/stock/financials?symbol={symbol}&statement=ic&freq=annual&token={api_key}"
        r = requests.get(url, timeout=timeout)
        if r.status_code == 200:
            js = r.json() or {}
            for item in (js.get("data") or [])[:years_to_fetch]:
                end = item.get("period") or item.get("endDate") or item.get("end")
                if not end:
                    continue
                rec = rows.setdefault(end, {"end": end, "source": "finnhub"})
                
                # Revenue
                rev = item.get("revenue") or item.get("totalRevenue") or item.get("Revenue")
                if rev is not None:
                    try:
                        rec["revenue"] = float(rev)
                    except:
                        pass
                
                # EPS Diluted
                eps = item.get("epsdiluted") or item.get("epsDiluted") or item.get("EPSDiluted")
                if eps is not None:
                    try:
                        rec["eps_diluted"] = float(eps)
                    except:
                        pass
    except Exception:
        pass
    
    if not rows:
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
    
    df = pd.DataFrame(list(rows.values()))
    df = df.sort_values("end", ascending=False).drop_duplicates("end", keep="first")
    return df


# ============================================================================
# Financial Modeling Prep (FMP) - Primary Validation Source
# ============================================================================

def fmp_annual_financials(symbol: str, api_key: str, years_to_fetch: int = 6, timeout: int = 15) -> pd.DataFrame:
    """
    Fetch annual revenue and EPS from Financial Modeling Prep API.
    
    FMP is used as part of the 3-source consensus validation system.
    With ~100 tickers and 250 calls/day, FMP provides reliable third-party 
    validation for all fallback data.
    
    Returns DataFrame with columns: end, revenue, eps_diluted, source
    """
    if not api_key:
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
    
    base = "https://financialmodelingprep.com/stable"
    rows = {}
    
    try:
        # Income statement endpoint provides revenue, net income, and EPS
        url = f"{base}/income-statement?symbol={symbol}&apikey={api_key}"
        r = requests.get(url, timeout=timeout)
        
        if r.status_code == 200:
            data = r.json()
            
            # Response is a list of annual reports, most recent first
            for item in (data or [])[:years_to_fetch]:
                # Only process annual (FY) periods
                period = item.get("period", "")
                if period != "FY":
                    continue
                
                # Get fiscal year end date
                end = item.get("date")  # Format: "2025-09-30"
                if not end:
                    continue
                
                rec = rows.setdefault(end, {"end": end, "source": "fmp"})
                
                # Revenue
                rev = item.get("revenue")
                if rev is not None:
                    try:
                        rec["revenue"] = float(rev)
                    except:
                        pass
                
                # EPS Diluted
                eps = item.get("epsDiluted")
                if eps is not None:
                    try:
                        rec["eps_diluted"] = float(eps)
                    except:
                        pass
        elif r.status_code == 429:
            print(f"[warn] FMP rate limit reached for {symbol}")
        else:
            print(f"[warn] FMP error for {symbol}: HTTP {r.status_code}")
            
    except Exception as e:
        print(f"[warn] FMP error for {symbol}: {e}")
    
    if not rows:
        return pd.DataFrame(columns=["end", "revenue", "eps_diluted", "source"])
    
    df = pd.DataFrame(list(rows.values()))
    df = df.sort_values("end", ascending=False).drop_duplicates("end", keep="first")
    return df


# ============================================================================
# Data Extraction Helpers
# ============================================================================

def is_annual_10k(row: dict) -> bool:
    """
    Check if a row is from an annual 10-K filing with full-year duration.
    
    This filters out restated quarterly values that appear in 10-K filings.
    A true annual record should have a period duration of approximately 12 months
    (at least 300 days to account for variations).
    """
    form = str(row.get("form", "")).upper()
    fp = str(row.get("fp", "")).upper()
    
    # Must be 10-K form with FY fiscal period
    if not (form.startswith("10-K") and (fp in ("FY", "") or fp.startswith("Q4"))):
        return False
    
    # Additionally check period duration (start to end)
    # Annual reports should cover ~12 months (at least 300 days)
    start = row.get("start")
    end = row.get("end")
    if start and end:
        try:
            from datetime import datetime
            start_dt = datetime.strptime(start, "%Y-%m-%d")
            end_dt = datetime.strptime(end, "%Y-%m-%d")
            days = (end_dt - start_dt).days
            # Must be at least 300 days (covers ~10-12 month fiscal years)
            if days < 300:
                return False
        except:
            pass
    
    return True


def is_quarterly_10q(row: dict) -> bool:
    """Check if a row is from a quarterly 10-Q filing."""
    form = str(row.get("form", "")).upper()
    fp = str(row.get("fp", "")).upper()
    return form.startswith("10-Q") and fp.startswith("Q")


def _is_eps_unit(k: str) -> bool:
    """Check if unit key is for per-share metrics."""
    s = k.lower().replace(" ", "").replace("-", "").replace("_", "")
    has_share = any(x in s for x in ["share", "shares", "shr", "shs", "/sh", "pershare"])
    has_usd = "usd" in s or "iso4217:usd" in s
    return has_share and has_usd


def _dedupe_by_end(rows: List[dict]) -> List[dict]:
    """Keep only the latest filing for each fiscal period end."""
    by_end = {}
    for r in rows:
        end = r.get("end")
        if not end:
            continue
        cur = by_end.get(end)
        if cur is None or r.get("filed", "") >= cur.get("filed", ""):
            by_end[end] = r
    
    result = sorted(by_end.values(), key=lambda x: x.get("end", ""), reverse=True)
    return result


def extract_concept_values(
    companyfacts: dict,
    concept_names: List[str],
    is_eps: bool = False,
    filter_func=None
) -> List[dict]:
    """
    Extract values for a list of concept names, selecting the concept with 
    the most recent data (not first match).
    
    This fixes issues where older concepts (like RevenueFromContractWithCustomer
    ExcludingAssessedTax) are listed first but have outdated data, while newer
    concepts (like Revenues) have current data.
    
    Returns list of {end, val, concept, filed} dicts.
    """
    facts = companyfacts.get("facts", {}) or {}
    
    # Collect data from ALL matching concepts, then pick best one
    all_concept_results = []
    
    for taxonomy in ("us-gaap", "dei"):
        sec = facts.get(taxonomy, {}) or {}
        for concept_name in concept_names:
            if concept_name not in sec:
                continue
            
            node = sec[concept_name]
            units = node.get("units", {}) or {}
            
            if is_eps:
                unit_keys = [k for k in units.keys() if _is_eps_unit(k)]
                if not unit_keys and "USD" in units:
                    unit_keys = ["USD"]
            else:
                unit_keys = ["USD"] if "USD" in units else []
            
            for unit_key in unit_keys:
                rows = units.get(unit_key, [])
                filtered_rows = []
                for r in rows:
                    if not r.get("start") or not r.get("end"):
                        continue  # Skip instant facts
                    if filter_func and not filter_func(r):
                        continue
                    filtered_rows.append({
                        "end": r.get("end"),
                        "val": r.get("val"),
                        "concept": concept_name,
                        "filed": r.get("filed", ""),
                    })
                
                if filtered_rows:
                    deduped = _dedupe_by_end(filtered_rows)
                    # Track the most recent end date for this concept
                    most_recent_end = max(r.get("end", "") for r in deduped)
                    all_concept_results.append({
                        "concept": concept_name,
                        "most_recent_end": most_recent_end,
                        "data": deduped
                    })
    
    if not all_concept_results:
        return []
    
    # Select the concept with the most recent fiscal year end date
    best_concept = max(all_concept_results, key=lambda x: x["most_recent_end"])
    
    # Log selection if we had multiple options
    if len(all_concept_results) > 1:
        logging.debug(
            f"Selected concept '{best_concept['concept']}' with most recent data "
            f"({best_concept['most_recent_end']}) over {len(all_concept_results)-1} other concepts"
        )
    
    return best_concept["data"]


# ============================================================================
# Growth Calculations
# ============================================================================

def calculate_yoy_growth(values: List[Optional[float]]) -> Dict[str, Optional[float]]:
    """Calculate YoY growth for consecutive periods."""
    result = {}
    for i in range(len(values) - 1):
        cur = values[i]
        prev = values[i + 1]
        
        year_label = f"period_{i}"
        
        if cur is not None and prev is not None and prev != 0:
            result[year_label] = round((cur / prev) - 1, 4)
        else:
            result[year_label] = None
    
    return result


def calculate_cagr(start_val: Optional[float], end_val: Optional[float], years: int) -> Optional[float]:
    """Calculate Compound Annual Growth Rate."""
    if start_val is None or end_val is None or start_val <= 0 or years <= 0:
        return None
    try:
        return round((end_val / start_val) ** (1 / years) - 1, 4)
    except Exception:
        return None


def calculate_ttm(quarters: List[QuarterlyRecord], metric: str) -> Optional[float]:
    """Sum the most recent 4 quarters for a metric."""
    if len(quarters) < 4:
        return None
    
    vals = []
    for q in quarters[:4]:
        v = getattr(q, metric, None)
        if v is None:
            return None
        vals.append(v)
    
    return sum(vals)


def calculate_ttm_yoy(quarters: List[QuarterlyRecord], metric: str) -> Optional[float]:
    """
    Calculate TTM YoY growth.
    Compares sum of quarters 0-3 (most recent TTM) to sum of quarters 4-7 (prior TTM).
    """
    if len(quarters) < 8:
        return None
    
    current_ttm = []
    prior_ttm = []
    
    for i, q in enumerate(quarters[:8]):
        v = getattr(q, metric, None)
        if v is None:
            return None
        if i < 4:
            current_ttm.append(v)
        else:
            prior_ttm.append(v)
    
    if len(current_ttm) < 4 or len(prior_ttm) < 4:
        return None
    
    current_sum = sum(current_ttm)
    prior_sum = sum(prior_ttm)
    
    if prior_sum == 0:
        return None
    
    return round((current_sum / prior_sum) - 1, 4)


# ============================================================================
# Main Extraction Logic
# ============================================================================

def extract_company_data(
    ticker: str,
    cik: str,
    companyfacts: dict,
    config: Config
) -> CompanyData:
    """Extract annual and quarterly financial data for a single company."""
    errors = []
    extracted_at = datetime.utcnow().isoformat() + "Z"
    
    # Get company name
    company_name = companyfacts.get("entityName") if companyfacts else None
    
    # ========== ANNUAL DATA (10-K) ==========
    revenue_rows = extract_concept_values(
        companyfacts,
        config.concepts.get("revenue", []),
        is_eps=False,
        filter_func=is_annual_10k
    )
    
    eps_rows = extract_concept_values(
        companyfacts,
        config.concepts.get("eps_diluted", []),
        is_eps=True,
        filter_func=is_annual_10k
    )
    
    revenue_rows = revenue_rows[:config.years_to_fetch]
    eps_rows = eps_rows[:config.years_to_fetch]
    
    if not revenue_rows:
        errors.append("No annual revenue data found")
    if not eps_rows:
        errors.append("No annual EPS data found")
    
    # Build annual records
    all_ends = set()
    revenue_by_end = {r["end"]: r for r in revenue_rows}
    eps_by_end = {r["end"]: r for r in eps_rows}
    all_ends.update(revenue_by_end.keys())
    all_ends.update(eps_by_end.keys())
    
    sorted_ends = sorted(all_ends, reverse=True)[:config.years_to_fetch]
    
    annual_data = []
    for end in sorted_ends:
        rev_rec = revenue_by_end.get(end)
        eps_rec = eps_by_end.get(end)
        
        annual_data.append(AnnualRecord(
            fiscal_year_end=end,
            revenue=rev_rec["val"] if rev_rec else None,
            eps_diluted=eps_rec["val"] if eps_rec else None,
            revenue_concept=rev_rec["concept"] if rev_rec else None,
            eps_concept=eps_rec["concept"] if eps_rec else None,
        ))
    
    # ========== ANNUAL FALLBACK WITH CROSS-VALIDATION ==========
    # Count how many annual records have null revenue or EPS
    annual_null_revenue = sum(1 for a in annual_data if a.revenue is None)
    annual_null_eps = sum(1 for a in annual_data if a.eps_diluted is None)
    # Also need fallback if we don't have enough years of data
    need_more_years = len(annual_data) < config.years_to_fetch
    need_annual_fallback = (annual_null_revenue > 0 or annual_null_eps > 0 or len(annual_data) == 0 or need_more_years)
    
    if need_annual_fallback:
        # Gather data from ALL available fallback sources for consensus-based validation
        # FMP is now included upfront for 2-out-of-3 consensus voting
        all_fallback_sources = gather_all_fallback_data(ticker, config, config.years_to_fetch)
        
        if all_fallback_sources:
            # For each annual record that needs fallback data, cross-validate
            for a in annual_data:
                # Revenue cross-validation
                if a.revenue is None:
                    rev_validation = cross_validate_fallback_year(
                        a.fiscal_year_end, 
                        all_fallback_sources, 
                        "revenue",
                        tolerance_pct=5.0
                    )
                    if rev_validation.value is not None:
                        a.revenue = rev_validation.value
                        a.revenue_concept = f"fallback:{','.join(rev_validation.sources_compared)}"
                        a.revenue_validation = rev_validation.status
                        a.revenue_sources = rev_validation.sources_compared
                        a.revenue_discrepancy_pct = rev_validation.discrepancy_pct
                
                # EPS cross-validation
                if a.eps_diluted is None:
                    eps_validation = cross_validate_fallback_year(
                        a.fiscal_year_end, 
                        all_fallback_sources, 
                        "eps_diluted",
                        tolerance_pct=5.0
                    )
                    if eps_validation.value is not None:
                        a.eps_diluted = eps_validation.value
                        a.eps_concept = f"fallback:{','.join(eps_validation.sources_compared)}"
                        a.eps_validation = eps_validation.status
                        a.eps_sources = eps_validation.sources_compared
                        a.eps_discrepancy_pct = eps_validation.discrepancy_pct
            
            # Add new years from fallback sources that we're missing entirely
            existing_years = {a.fiscal_year_end[:4] for a in annual_data}
            
            # Collect all unique year dates from all sources
            all_year_dates = {}
            for source_name, df in all_fallback_sources.items():
                for _, row in df.iterrows():
                    end = row.get("end")
                    if end:
                        year = end[:4]
                        if year not in existing_years:
                            all_year_dates[year] = end
            
            # Add missing years with cross-validation (consensus-based)
            for year, year_date in all_year_dates.items():
                if year not in existing_years:
                    rev_validation = cross_validate_fallback_year(year_date, all_fallback_sources, "revenue", 5.0)
                    eps_validation = cross_validate_fallback_year(year_date, all_fallback_sources, "eps_diluted", 5.0)
                    
                    if rev_validation.value is not None or eps_validation.value is not None:
                        annual_data.append(AnnualRecord(
                            fiscal_year_end=year_date,
                            revenue=rev_validation.value,
                            eps_diluted=eps_validation.value,
                            revenue_concept=f"fallback:{','.join(rev_validation.sources_compared)}" if rev_validation.sources_compared else None,
                            eps_concept=f"fallback:{','.join(eps_validation.sources_compared)}" if eps_validation.sources_compared else None,
                            revenue_validation=rev_validation.status if rev_validation.value else None,
                            eps_validation=eps_validation.status if eps_validation.value else None,
                            revenue_sources=rev_validation.sources_compared if rev_validation.value else None,
                            eps_sources=eps_validation.sources_compared if eps_validation.value else None,
                            revenue_discrepancy_pct=rev_validation.discrepancy_pct,
                            eps_discrepancy_pct=eps_validation.discrepancy_pct,
                        ))
                        existing_years.add(year)
            
            # Re-sort and limit
            annual_data.sort(key=lambda x: x.fiscal_year_end, reverse=True)
            annual_data = annual_data[:config.years_to_fetch]
    
    # ========== QUARTERLY DATA (10-Q) ==========
    quarterly_data = []
    quarterly_source = "sec"
    
    # Try SEC first
    q_revenue_rows = extract_concept_values(
        companyfacts,
        config.concepts.get("revenue", []),
        is_eps=False,
        filter_func=is_quarterly_10q
    )
    
    q_eps_rows = extract_concept_values(
        companyfacts,
        config.concepts.get("eps_diluted", []),
        is_eps=True,
        filter_func=is_quarterly_10q
    )
    
    q_revenue_rows = q_revenue_rows[:config.quarters_to_fetch]
    q_eps_rows = q_eps_rows[:config.quarters_to_fetch]
    
    # Build quarterly from SEC
    q_all_ends = set()
    q_revenue_by_end = {r["end"]: r for r in q_revenue_rows}
    q_eps_by_end = {r["end"]: r for r in q_eps_rows}
    q_all_ends.update(q_revenue_by_end.keys())
    q_all_ends.update(q_eps_by_end.keys())
    
    q_sorted_ends = sorted(q_all_ends, reverse=True)[:config.quarters_to_fetch]
    
    for end in q_sorted_ends:
        rev_rec = q_revenue_by_end.get(end)
        eps_rec = q_eps_by_end.get(end)
        
        quarterly_data.append(QuarterlyRecord(
            fiscal_quarter_end=end,
            revenue=rev_rec["val"] if rev_rec else None,
            eps_diluted=eps_rec["val"] if eps_rec else None,
            source="sec",
        ))
    
    # ========== QUARTERLY FALLBACK (yfinance first, then Finnhub) ==========
    # Check if SEC quarterly data is incomplete for TTM calculation.
    # We need the MOST RECENT 4 quarters to have both revenue and EPS for accurate TTM.
    # Also trigger fallback if ANY quarter in the top 8 is missing data (helps CAGR calculations).
    recent_4 = quarterly_data[:4] if len(quarterly_data) >= 4 else quarterly_data
    recent_4_revenue = sum(1 for q in recent_4 if q.revenue is not None)
    recent_4_eps = sum(1 for q in recent_4 if q.eps_diluted is not None)
    
    any_null_revenue = any(q.revenue is None for q in quarterly_data)
    any_null_eps = any(q.eps_diluted is None for q in quarterly_data)
    
    # Trigger fallback if: most recent 4 quarters are incomplete OR any quarter has nulls
    need_quarterly_fallback = (recent_4_revenue < 4 or recent_4_eps < 4 or any_null_revenue or any_null_eps)
    quarterly_source = "sec"
    
    def merge_quarterly_fallback(df: pd.DataFrame, source_name: str):
        """Helper to merge fallback data into quarterly_data."""
        nonlocal quarterly_source
        if df.empty:
            return
        
        # Build lookup by date
        fb_by_end = {}
        for _, row in df.iterrows():
            end = row.get("end")
            if end:
                fb_by_end[end] = row
        
        # Fill gaps in existing quarters
        for q in quarterly_data:
            fb_rec = fb_by_end.get(q.fiscal_quarter_end)
            if fb_rec is not None:
                if q.revenue is None and pd.notna(fb_rec.get("revenue")):
                    q.revenue = float(fb_rec["revenue"])
                    q.source = source_name
                    quarterly_source = "mixed"
                if q.eps_diluted is None and pd.notna(fb_rec.get("eps_diluted")):
                    q.eps_diluted = float(fb_rec["eps_diluted"])
                    q.source = source_name
                    quarterly_source = "mixed"
        
        # Add any new quarters not in SEC
        existing_ends = {q.fiscal_quarter_end for q in quarterly_data}
        for end, row in fb_by_end.items():
            if end not in existing_ends:
                quarterly_data.append(QuarterlyRecord(
                    fiscal_quarter_end=end,
                    revenue=float(row["revenue"]) if pd.notna(row.get("revenue")) else None,
                    eps_diluted=float(row["eps_diluted"]) if pd.notna(row.get("eps_diluted")) else None,
                    source=source_name,
                ))
                quarterly_source = "mixed"
    
    if need_quarterly_fallback:
        # 1. Try yfinance first (free, no API key needed)
        if config.yfinance_enabled:
            yf_quarterly = yfinance_quarterly_financials(ticker, config.quarters_to_fetch)
            merge_quarterly_fallback(yf_quarterly, "yfinance")
        
        # Recheck if still need fallback
        current_revenue = sum(1 for q in quarterly_data if q.revenue is not None)
        current_eps = sum(1 for q in quarterly_data if q.eps_diluted is not None)
        still_need_fallback = (current_revenue < 4 or current_eps < 4)
        
        # 2. Try Finnhub if still incomplete
        if still_need_fallback and config.finnhub_enabled and config.finnhub_api_key:
            finnhub_df = finnhub_quarterly_financials(ticker, config.finnhub_api_key)
            merge_quarterly_fallback(finnhub_df, "finnhub")
        
        # Re-sort and limit
        quarterly_data.sort(key=lambda x: x.fiscal_quarter_end, reverse=True)
        quarterly_data = quarterly_data[:config.quarters_to_fetch]
    
    # ========== GROWTH METRICS ==========
    revenues = [a.revenue for a in annual_data]
    eps_values = [a.eps_diluted for a in annual_data]
    
    revenue_yoy = calculate_yoy_growth(revenues)
    eps_yoy = calculate_yoy_growth(eps_values)
    
    # Relabel with actual years
    def relabel_yoy(yoy_dict: dict, records: List[AnnualRecord]) -> Dict[str, Optional[float]]:
        result = {}
        for i, (key, val) in enumerate(yoy_dict.items()):
            if i < len(records):
                year = records[i].fiscal_year_end[:4]
                result[year] = val
        return result
    
    revenue_yoy = relabel_yoy(revenue_yoy, annual_data)
    eps_yoy = relabel_yoy(eps_yoy, annual_data)
    
    # ========== TTM METRICS (from quarterly) ==========
    ttm = None
    
    if len(quarterly_data) >= 4:
        ttm_revenue = calculate_ttm(quarterly_data, "revenue")
        ttm_eps = calculate_ttm(quarterly_data, "eps_diluted")
        ttm_revenue_yoy = calculate_ttm_yoy(quarterly_data, "revenue")
        ttm_eps_yoy = calculate_ttm_yoy(quarterly_data, "eps_diluted")
        
        # Determine source
        sources = set(q.source for q in quarterly_data[:4])
        if "finnhub" in sources and "sec" in sources:
            ttm_source = "mixed"
        elif "finnhub" in sources:
            ttm_source = "finnhub"
        else:
            ttm_source = "sec"
        
        ttm = TTMMetrics(
            revenue=ttm_revenue,
            eps_diluted=ttm_eps,
            revenue_yoy=ttm_revenue_yoy,
            eps_yoy=ttm_eps_yoy,
            as_of_quarter=quarterly_data[0].fiscal_quarter_end if quarterly_data else None,
            source=ttm_source,
        )
    elif annual_data:
        # Fallback: use most recent annual as TTM approximation
        ttm_revenue_yoy = None
        ttm_eps_yoy = None
        if len(revenues) >= 2 and revenues[0] is not None and revenues[1] is not None and revenues[1] != 0:
            ttm_revenue_yoy = round((revenues[0] / revenues[1]) - 1, 4)
        if len(eps_values) >= 2 and eps_values[0] is not None and eps_values[1] is not None and eps_values[1] != 0:
            ttm_eps_yoy = round((eps_values[0] / eps_values[1]) - 1, 4)
        
        ttm = TTMMetrics(
            revenue=revenues[0] if revenues else None,
            eps_diluted=eps_values[0] if eps_values else None,
            revenue_yoy=ttm_revenue_yoy,
            eps_yoy=ttm_eps_yoy,
            as_of_quarter=annual_data[0].fiscal_year_end if annual_data else None,
            source="annual_fallback",
        )
    
    # ========== CAGR ==========
    revenue_cagr_3yr = None
    eps_cagr_3yr = None
    if len(revenues) >= 4 and revenues[0] is not None and revenues[3] is not None:
        revenue_cagr_3yr = calculate_cagr(revenues[3], revenues[0], 3)
    if len(eps_values) >= 4 and eps_values[0] is not None and eps_values[3] is not None:
        eps_cagr_3yr = calculate_cagr(eps_values[3], eps_values[0], 3)
    
    revenue_cagr_5yr = None
    eps_cagr_5yr = None
    if len(revenues) >= 6 and revenues[0] is not None and revenues[5] is not None:
        revenue_cagr_5yr = calculate_cagr(revenues[5], revenues[0], 5)
    if len(eps_values) >= 6 and eps_values[0] is not None and eps_values[5] is not None:
        eps_cagr_5yr = calculate_cagr(eps_values[5], eps_values[0], 5)
    
    growth = GrowthMetrics(
        revenue_yoy=revenue_yoy,
        eps_yoy=eps_yoy,
        ttm=ttm,
        revenue_cagr_3yr=revenue_cagr_3yr,
        eps_cagr_3yr=eps_cagr_3yr,
        revenue_cagr_5yr=revenue_cagr_5yr,
        eps_cagr_5yr=eps_cagr_5yr,
    )
    
    return CompanyData(
        ticker=ticker,
        cik=cik,
        company_name=company_name,
        extracted_at=extracted_at,
        annual_data=annual_data,
        quarterly_data=quarterly_data,
        growth=growth,
        errors=errors,
    )


def company_data_to_dict(data: CompanyData) -> dict:
    """Convert CompanyData to JSON-serializable dict."""
    return {
        "ticker": data.ticker,
        "cik": data.cik,
        "company_name": data.company_name,
        "extracted_at": data.extracted_at,
        "growth": {
            "revenue_yoy": data.growth.revenue_yoy,
            "eps_yoy": data.growth.eps_yoy,
            "ttm": asdict(data.growth.ttm) if data.growth.ttm else None,
            "revenue_cagr_3yr": data.growth.revenue_cagr_3yr,
            "eps_cagr_3yr": data.growth.eps_cagr_3yr,
            "revenue_cagr_5yr": data.growth.revenue_cagr_5yr,
            "eps_cagr_5yr": data.growth.eps_cagr_5yr,
        },
        "errors": data.errors,
    }


# ============================================================================
# I/O
# ============================================================================

def save_json(data: dict, path: Path, indent: int = 2):
    """Save dict to JSON file."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w") as f:
        json.dump(data, f, indent=indent)


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Extract annual & quarterly revenue/EPS from SEC EDGAR for S&P 100 companies."
    )
    parser.add_argument("--config", "-c", default=None, help="Path to config.yml")
    parser.add_argument("--output", "-o", default=None, help="Output directory")
    args = parser.parse_args()
    
    # Load config
    cwd = Path(__file__).parent
    config_path = Path(args.config) if args.config else (cwd / "config.yml")
    if not config_path.exists():
        raise SystemExit(f"Config not found: {config_path}")
    
    config = Config.from_yaml(str(config_path))
    
    if args.output:
        config.output_dir = Path(args.output)
    
    # Get S&P 100 tickers from universe
    print("[info] Fetching S&P 100 universe...")
    tickers = fetch_sp100_tickers()
    
    if not tickers:
        raise SystemExit("No tickers to process - S&P 100 universe is empty")
    
    print(f"[info] Processing {len(tickers)} S&P 100 ticker(s)")
    print(f"[info] Output: {config.output_dir / config.output_filename}")
    print(f"[info] Annual fallbacks: yfinance={'enabled' if config.yfinance_enabled else 'disabled'}, Alpha Vantage={'enabled' if config.alphavantage_enabled and config.alphavantage_api_key else 'disabled'}, Finnhub={'enabled' if config.finnhub_enabled and config.finnhub_api_key else 'disabled'}")
    print(f"[info] Quarterly fallback: Finnhub={'enabled' if config.finnhub_enabled and config.finnhub_api_key else 'disabled'}")
    
    # Load CIK mapping
    print("[info] Loading SEC CIK mapping...")
    ticker_to_cik = load_ticker_to_cik(config.user_agent)
    
    # Process tickers
    results = []
    success_count = 0
    
    for i, ticker in enumerate(tickers, 1):
        print(f"[{i}/{len(tickers)}] {ticker}...", end=" ", flush=True)
        
        cik = ticker_to_cik.get(ticker, "")
        if not cik:
            print("no CIK found, skipping")
            results.append(CompanyData(
                ticker=ticker, cik="", company_name=None,
                extracted_at=datetime.utcnow().isoformat() + "Z",
                annual_data=[], quarterly_data=[],
                growth=GrowthMetrics({}, {}),
                errors=["CIK not found"],
            ))
            continue
        
        facts = fetch_company_facts(ticker, config.user_agent)
        if not facts:
            print("no SEC data, skipping")
            results.append(CompanyData(
                ticker=ticker, cik=cik, company_name=None,
                extracted_at=datetime.utcnow().isoformat() + "Z",
                annual_data=[], quarterly_data=[],
                growth=GrowthMetrics({}, {}),
                errors=["Failed to fetch SEC data"],
            ))
            continue
        
        company_data = extract_company_data(ticker, cik, facts, config)
        results.append(company_data)
        
        if company_data.annual_data:
            success_count += 1
            years = len(company_data.annual_data)
            quarters = len(company_data.quarterly_data)
            q_source = company_data.growth.ttm.source if company_data.growth.ttm else "none"
            print(f"OK ({years} years, {quarters} quarters, TTM: {q_source})")
        else:
            print("no data found")
    
    # Save output
    config.output_dir.mkdir(parents=True, exist_ok=True)
    
    # Build _README section (following project standards)
    readme_section = {
        "title": "S&P 100 Growth Metrics",
        "description": "Financial growth metrics extracted from SEC EDGAR filings for S&P 100 companies",
        "purpose": "Track revenue and EPS growth for the largest US companies",
        "data_sources": {
            "primary": "SEC EDGAR (10-K and 10-Q filings)",
            "annual_fallbacks": [
                "yfinance (Yahoo Finance)",
                "Alpha Vantage API",
                "Finnhub API"
            ],
            "quarterly_fallback": "Finnhub API"
        },
        "metrics_explained": {
            "revenue_yoy": {
                "description": "Year-over-year revenue growth rate",
                "format": "Decimal (0.05 = 5% growth, -0.03 = 3% decline)",
                "calculation": "(current_year_revenue / prior_year_revenue) - 1"
            },
            "eps_yoy": {
                "description": "Year-over-year diluted EPS growth rate",
                "format": "Decimal (0.10 = 10% growth)",
                "calculation": "(current_year_eps / prior_year_eps) - 1"
            },
            "ttm": {
                "description": "Trailing Twelve Months metrics from the most recent 4 quarters",
                "fields": {
                    "revenue": "Sum of revenue from the last 4 quarters (USD)",
                    "eps_diluted": "Sum of diluted EPS from the last 4 quarters",
                    "revenue_yoy": "TTM revenue growth vs prior TTM",
                    "eps_yoy": "TTM EPS growth vs prior TTM",
                    "source": "'sec', 'finnhub', 'mixed', or 'annual_fallback'"
                }
            },
            "cagr_3yr": {
                "description": "3-year Compound Annual Growth Rate",
                "calculation": "(end_value / start_value)^(1/3) - 1"
            },
            "cagr_5yr": {
                "description": "5-year Compound Annual Growth Rate",
                "calculation": "(end_value / start_value)^(1/5) - 1"
            }
        },
        "concept_sources": {
            "sec": "Direct SEC XBRL concept extraction",
            "fallback:yfinance": "Yahoo Finance annual data via yfinance",
            "fallback:alphavantage": "Alpha Vantage fundamental data API",
            "fallback:finnhub": "Finnhub stock fundamentals API"
        },
        "trading_applications": {
            "growth_screening": "Filter companies by revenue/EPS growth rates",
            "momentum_analysis": "Track acceleration/deceleration in fundamentals",
            "valuation_context": "Compare growth rates to P/E ratios",
            "sector_comparison": "Identify growth leaders within sectors"
        },
        "notes": [
            "All growth rates are decimals (multiply by 100 for percentage)",
            "Null values indicate insufficient data from all sources",
            "Revenue figures are in USD (not scaled)",
            "Fiscal year end dates vary by company",
            "revenue_concept and eps_concept fields indicate data source"
        ]
    }
    
    # Build metadata
    metadata = {
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "data_sources": "SEC EDGAR + yfinance + Alpha Vantage + Finnhub",
        "ticker_count": len(results),
        "successful_extractions": success_count,
        "universe": "S&P 100"
    }
    
    # Build companies dict keyed by ticker
    companies = {}
    for result in results:
        companies[result.ticker] = company_data_to_dict(result)
    
    combined = {
        "_README": readme_section,
        "metadata": metadata,
        "companies": companies,
    }
    
    out_path = config.output_dir / config.output_filename
    save_json(combined, out_path, config.indent)
    print(f"\n[ok] Wrote output to {out_path}")
    
    print(f"[ok] Successfully extracted data for {success_count}/{len(tickers)} tickers")


if __name__ == "__main__":
    main()
